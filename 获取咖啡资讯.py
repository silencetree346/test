#!/usr/bin/env python3
import asyncio
import httpx
import feedparser
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import json

class CoffeeNewsCollector:
    def __init__(self):
        self.news_items = []
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        }
    
    async def fetch_36kr(self):
        """è·å–36æ°ªRSS"""
        try:
            feed_url = "https://36kr.com/feed"
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(feed_url, headers=self.headers)
                feed = feedparser.parse(response.text)
                
                count = 0
                for entry in feed.entries[:20]:
                    title = entry.get('title', '')
                    link = entry.get('link', '')
                    published = entry.get('published', '')
                    summary = entry.get('summary', '')
                    
                    if self._is_coffee_related(title + ' ' + summary):
                        self.news_items.append({
                            'title': title,
                            'url': link,
                            'source': '36æ°ª',
                            'date': published,
                            'summary': BeautifulSoup(summary, 'html.parser').get_text()[:200]
                        })
                        count += 1
                
                print(f"âœ… 36æ°ª: è·å–åˆ° {count} æ¡å’–å•¡ç›¸å…³èµ„è®¯")
        except Exception as e:
            print(f"âŒ 36æ°ªè·å–å¤±è´¥: {str(e)}")
    
    async def fetch_huxiu(self):
        """è·å–è™å—…RSS"""
        try:
            feed_url = "https://www.huxiu.com/rss/0.xml"
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(feed_url, headers=self.headers)
                feed = feedparser.parse(response.text)
                
                count = 0
                for entry in feed.entries[:20]:
                    title = entry.get('title', '')
                    link = entry.get('link', '')
                    published = entry.get('published', '')
                    summary = entry.get('summary', '')
                    
                    if self._is_coffee_related(title + ' ' + summary):
                        self.news_items.append({
                            'title': title,
                            'url': link,
                            'source': 'è™å—…',
                            'date': published,
                            'summary': BeautifulSoup(summary, 'html.parser').get_text()[:200]
                        })
                        count += 1
                
                print(f"âœ… è™å—…: è·å–åˆ° {count} æ¡å’–å•¡ç›¸å…³èµ„è®¯")
        except Exception as e:
            print(f"âŒ è™å—…è·å–å¤±è´¥: {str(e)}")
    
    async def fetch_tmtpost(self):
        """è·å–é’›åª’ä½“RSS"""
        try:
            feed_url = "https://www.tmtpost.com/rss.xml"
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(feed_url, headers=self.headers)
                feed = feedparser.parse(response.text)
                
                count = 0
                for entry in feed.entries[:20]:
                    title = entry.get('title', '')
                    link = entry.get('link', '')
                    published = entry.get('published', '')
                    summary = entry.get('summary', '')
                    
                    if self._is_coffee_related(title + ' ' + summary):
                        self.news_items.append({
                            'title': title,
                            'url': link,
                            'source': 'é’›åª’ä½“',
                            'date': published,
                            'summary': BeautifulSoup(summary, 'html.parser').get_text()[:200]
                        })
                        count += 1
                
                print(f"âœ… é’›åª’ä½“: è·å–åˆ° {count} æ¡å’–å•¡ç›¸å…³èµ„è®¯")
        except Exception as e:
            print(f"âŒ é’›åª’ä½“è·å–å¤±è´¥: {str(e)}")
    
    def _is_coffee_related(self, text):
        """æ£€æŸ¥æ˜¯å¦ä¸ºå’–å•¡ç›¸å…³å†…å®¹"""
        keywords = [
            'å’–å•¡', 'æ˜Ÿå·´å…‹', 'ç‘å¹¸', 'luckin', 'starbucks',
            'manner', 'seesaw', 'costa', 'é›€å·¢', 
            'coffee', 'cafe', 'espresso', 'latte'
        ]
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in keywords)
    
    async def collect_all_news(self):
        """å¹¶å‘è·å–æ‰€æœ‰æ–°é—»æº"""
        print("ğŸ”„ å¼€å§‹è·å–å’–å•¡è¡Œä¸šèµ„è®¯...\n")
        
        tasks = [
            self.fetch_36kr(),
            self.fetch_huxiu(),
            self.fetch_tmtpost()
        ]
        
        await asyncio.gather(*tasks, return_exceptions=True)
        
        print(f"\nğŸ“Š å…±è·å–åˆ° {len(self.news_items)} æ¡å’–å•¡ç›¸å…³èµ„è®¯")
        
        return self.news_items
    
    def display_results(self):
        """æ˜¾ç¤ºç»“æœ"""
        if not self.news_items:
            print("\nâš ï¸  æœªè·å–åˆ°å’–å•¡ç›¸å…³èµ„è®¯")
            return
        
        print("\n" + "="*80)
        print(f"ğŸ“° å’–å•¡è¡Œä¸šèµ„è®¯æ±‡æ€» - {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}")
        print("="*80 + "\n")
        
        for idx, item in enumerate(self.news_items, 1):
            print(f"{idx}. ã€{item['source']}ã€‘{item['title']}")
            print(f"   ğŸ”— {item['url']}")
            if item.get('summary'):
                print(f"   ğŸ’¬ {item['summary']}")
            print()
        
        self.save_to_file()
    
    def save_to_file(self):
        """ä¿å­˜åˆ°æ–‡ä»¶"""
        filename = f"å’–å•¡èµ„è®¯_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.news_items, f, ensure_ascii=False, indent=2)
        print(f"âœ… èµ„è®¯å·²ä¿å­˜åˆ°: {filename}")

async def main():
    collector = CoffeeNewsCollector()
    await collector.collect_all_news()
    collector.display_results()

if __name__ == "__main__":
    asyncio.run(main())

